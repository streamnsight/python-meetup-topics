{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is a discipline that has been developped over many decades now, and has become an important piece of the puzzle to solve challenging text-based AI problems, such classifying text documents, generating text, language understanding, chatbots, speech and more...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology\n",
    "We'll go over the typical terminology used in NLP, and the tools used to decompose NLP problems.\n",
    "\n",
    "### Methods\n",
    "- Tokenizing\n",
    "- Part Of Speech (POS) tagging\n",
    "- Stemming & Lemmatization\n",
    "- Stop Words\n",
    "- Name Entity Recognition (NER)\n",
    "- Dependency Parsing\n",
    "- Vocabulary\n",
    "- Embeddings\n",
    "\n",
    "also:\n",
    "- Bag Of Words (BOW)\n",
    "- bigrams, trigrams, ngrams\n",
    "- skip-ngram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing & Tokens\n",
    "Text problems begin with text. A text document is composed of phrases, themselves composed of words and punctuation.\n",
    "\n",
    "In order to start analyzing text, one usually needs to decompose the text into it's basic components: sentences, and then words.\n",
    "\n",
    "Splitting sentences is not always trivial: simply splitting on `.` or capitalized first word just won't cut it in the real world.\n",
    "\n",
    "Then, words come in many flavors, or are not even words, like abbreviations (U.K., U.S.A), contractions ('s in \"it's good\"), or hyphenated words (like \"on-time\", \"mother-in-law\"), so basic components of text are generally called 'tokens'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "Tokenizing is the action of splitting text into tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many frameworks for NLP out there. One of the most popular is `nltk` (Natural Language Tool Kit), \n",
    "which is very versatile and geared towards research.\n",
    "\n",
    "`spaCy` is another one which is much more opinionated and targetting engineers for 'production ready NLP'\n",
    "\n",
    "`scikit-learn` is a more generic data science framework that includes NLP functionalities.\n",
    "\n",
    "Then with Deep Learning came a flurry of other libraries focusing on deep learning methods, yet typically the text decomposition \n",
    "is done using the libraries above.\n",
    "\n",
    "Neural Network based libraries include `gensim` which implements the popular Word2Vec model (more on this later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries. \n",
    "And sometimes sentences can start with non-capitalized words.  i is a good variable name.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the punkt package, if not already loaded, use:\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Tokenizing with nltk\n",
    "from nltk.data import load\n",
    "\n",
    "# Tokenize sentences (i.e. split on proper punctuation)\n",
    "\n",
    "sent_detector = load('nltk:tokenizers/punkt/english.pickle')\n",
    "tokens = sent_detector.tokenize(text)\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f'Sentence {i}: {token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words:\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences with spaCy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for i, token in enumerate(doc.sents):\n",
    "    print(f'Sentence {i}: {token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words with spaCy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for i, token in enumerate(doc):\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output may be slightly different. SpaCy didn't parse `non-capitlized` into a single token, and kept the newline characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging: Part Of Speech Tagging (POS tagging)\n",
    "\n",
    "Part Of Speech (POS) tagging is a somewhat standard method to recognize token types (nouns, verbs, adjectives, punctuation etc..)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging with nltk\n",
    "import nltk\n",
    "\n",
    "# if data not loaded, load with:\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The full list and their meaning is:\n",
    "    \n",
    "- CC\tcoordinating conjunction\n",
    "- CD\tcardinal digit\n",
    "- DT\tdeterminer\n",
    "- EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "- FW\tforeign word\n",
    "- IN\tpreposition/subordinating conjunction\n",
    "- JJ\tadjective\t'big'\n",
    "- JJR\tadjective, comparative\t'bigger'\n",
    "- JJS\tadjective, superlative\t'biggest'\n",
    "- LS\tlist marker\t1)\n",
    "- MD\tmodal\tcould, will\n",
    "- NN\tnoun, singular 'desk'\n",
    "- NNS\tnoun plural\t'desks'\n",
    "- NNP\tproper noun, singular\t'Harrison'\n",
    "- NNPS\tproper noun, plural\t'Americans'\n",
    "- PDT\tpredeterminer\t'all the kids'\n",
    "- POS\tpossessive ending\tparent's\n",
    "- PRP\tpersonal pronoun\tI, he, she\n",
    "- PRP`$`\tpossessive pronoun\tmy, his, hers\n",
    "- RB\tadverb\tvery, silently,\n",
    "- RBR\tadverb, comparative\tbetter\n",
    "- RBS\tadverb, superlative\tbest\n",
    "- RP\tparticle\tgive up\n",
    "- TO\tto\tgo 'to' the store.\n",
    "- UH\tinterjection\terrrrrrrrm\n",
    "- VB\tverb, base form\ttake\n",
    "- VBD\tverb, past tense\ttook\n",
    "- VBG\tverb, gerund/present participle\ttaking\n",
    "- VBN\tverb, past participle\ttaken\n",
    "- VBP\tverb, sing. present, non-3d\ttake\n",
    "- VBZ\tverb, 3rd person sing. present\ttakes\n",
    "- WDT\twh-determiner\twhich\n",
    "- WP\twh-pronoun\twho, what\n",
    "- WP`$`\tpossessive wh-pronoun\twhose\n",
    "- WRB\twh-abverb\twhere, when\n",
    "\n",
    "from: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging with spaCy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for i, token in enumerate(doc):\n",
    "    print(f'{token.text} -> {token.pos_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we do with POS tags?\n",
    "\n",
    "In most ML models, only important words are used, otherwise the model vocabulary is so large that it is difficult to train a general model\n",
    "\n",
    "Many will start by using just nouns/proper nouns and verbs, getting rid of all other words and punctuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with spaCy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "doc = nlp(text)\n",
    "useful_tokens = [t.text for t in doc if t.pos_ in ['NOUN', 'VERB', 'PROPN']]\n",
    "print(useful_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some other cases, we might know that the text of interest is only nouns, or verbs.\n",
    "\n",
    "In most cases, POS tagging is used to remove white space and punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "A very common concept in NLP is removing stop words.\n",
    "\n",
    "Stop words are words that are so common that they do not bring any useful information when included in\n",
    "typical classification methods because they are just everywhere.\n",
    "\n",
    "A very common step is therefore to remove stop words, and NLP packages include a curated list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words in nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "without_stopwords = [t for t in tokens if t.lower() not in stopwords.words('english')]\n",
    "print(without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with spaCy\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print(STOP_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy tags the tokens with a flag `is_stop` that can be used more readily for filtering\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(text)\n",
    "print([t.text for t in doc if not t.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "Stemming is somewhat of a crude method that tries to chop the end of the word down to a root, while Lemmatization usually refers to using a vocabulary and morphological analysis of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"This was a challenging computational challenge for the computer challenger: \n",
    "optimizing the computed optimized optimization\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wth nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "tokens = word_tokenize(text)\n",
    "for token in tokens:\n",
    "    print(ps.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "for token in tokens:\n",
    "    print(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with spaCy\n",
    "\n",
    "# spaCy DOES NOT DO STEMMING\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(text)\n",
    "print([t.lemma_ for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Entity Recognition (NER)\n",
    "\n",
    "Name Entity recognition is a functionality that aims at detecting entities in text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Hackers are exploiting vulnerable Jira and Exim servers with the end goal of infecting \n",
    "them with a new Watchbog Linux Trojan variant and using the resulting botnet \n",
    "as part of a Monero cryptomining operation.\n",
    "\n",
    "European authorities fined Google a record $5.1 billion on Wednesday \n",
    "for abusing its power in the mobile phone market \n",
    "and ordered the company to alter its practices\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp(text)\n",
    "print([(t.text, t.label_) for t in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Autonomous cars shift insurance liability toward manufacturers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(text)\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f'{chunk.text} -> {chunk.root.text} -> {chunk.root.dep_} -> {chunk.root.head.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "The Vocabulary in a dataset, is the list of unique tokens in the dataset.\n",
    "\n",
    "If the dataset is not filtered after tokenization, it amounts to all words and punctuation.\n",
    "\n",
    "The larger the vocabulary the more likely some tokens are just adding to the noise, hence the need to filter the dataset tokens, \n",
    "removing punctuation that usually doesn't bring any information, stop words that are too common to be meaningful, and filtering only for words that may be meaningful,\n",
    "stemming or lemmatizing the remaining tokens to further reduce the vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Frequency\n",
    "\n",
    "Token frequency can be a useful piece of information. It can also be used to further reduce the vocabulary, by picking only\n",
    "tokens that appear enough times in the dataset to be meaningful.\n",
    "\n",
    "In order to do this, we need to count occurences. This is easily done with the Python `Counter` class, part of the `collections` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "all_text = ''\n",
    "line_count = 0\n",
    "tokens = []\n",
    "with open('big.txt', 'r') as f:\n",
    "    text = ''\n",
    "    for line in f.readlines():\n",
    "        text += line\n",
    "        line_count += 1\n",
    "        if line_count % 1000 == 0:\n",
    "            doc = nlp(text)\n",
    "            tokens += [t.text for t in doc if not t.is_stop and not t.is_punct and not t.is_space]\n",
    "            all_text += text\n",
    "            text = ''\n",
    "        if line_count > 5000:\n",
    "            break\n",
    "\n",
    "token_freq = Counter(tokens)\n",
    "\n",
    "# Note: this is a quick hack to load a large file, but this may split sentences mid-way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to further reduce the vocabulary, and capitalization doesn't bring additional information\n",
    "# it is often useful to lowercase all the words\n",
    "\n",
    "token_freq = Counter([t.lower() for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the vocabulary in a model\n",
    "\n",
    "The vocabulary is composed of tokens. Those tokens are words, so they are difficult to work with for a regression model\n",
    "or any neural network that expects numbers only.\n",
    "\n",
    "A very basic way to encode the vocabulary is to use an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary index\n",
    "\n",
    "vocab_index = {}\n",
    "for token in token_freq.keys():\n",
    "    vocab_index[token] = len(vocab_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to a model would then consist in a series of numbers, indices of the words of the sentence in the index\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'the quick brown fox jumps over the lazy dog'\n",
    "\n",
    "doc = nlp(text)\n",
    "tokens = [t.text for t in doc if not t.is_punct and not t.is_space and not t.is_stop]\n",
    "print(tokens)\n",
    "token_idx = [vocab_index.get(t, -1) for t in tokens]\n",
    "print(token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that if the words are not in the dictionary, they don't return a meaningful index.\n",
    "Ideally the vocabulary is built from the dataset, to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = all_text[:301]\n",
    "print(text)\n",
    "\n",
    "doc = nlp(text)\n",
    "tokens = [t.text.lower() for t in doc if not t.is_punct and not t.is_space and not t.is_stop]\n",
    "print(tokens)\n",
    "token_idx = [vocab_index.get(t, -1) for t in tokens]\n",
    "print(token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "#### 1-hot encoder\n",
    "\n",
    "The index method is not very useful directly as a model might interpret this numerical index as an ordering of the words.\n",
    "\n",
    "here `ebook` as index 2 and `country` has index 18. A computer model would interpret this as `country` having 9x more weight\n",
    "than ebook, while `project` with index 0, would not even count.\n",
    "\n",
    "Since tokens are values of a category, they usually need to be transformed into a vector, where all values are 0, excpet for the \n",
    "column of the word index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 hot encoding of vocabulary\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf) # this is so the full array can be displayed\n",
    "\n",
    "def one_hot_encoder(token):\n",
    "    vector = np.zeros(len(vocab_index))\n",
    "    index = vocab_index.get(token, -1)\n",
    "    if index != -1:\n",
    "        vector[index] = 1.0\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = all_text[:301]\n",
    "print(text)\n",
    "\n",
    "doc = nlp(text)\n",
    "tokens = [t.text.lower() for t in doc if not t.is_punct and not t.is_space and not t.is_stop]\n",
    "print(tokens)\n",
    "token_vectors = np.array([one_hot_encoder(t) for t in tokens])\n",
    "print(token_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this is not very efficient, as the array for each word is the size of the vocabulary.\n",
    "\n",
    "To circumvent this issue, one can use embeddings that have reduced dimentionality, like Word2Vec or GLoVe\n",
    "\n",
    "spaCy also provide this functionality by default, using the vector attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "text = all_text[:301]\n",
    "print(text)\n",
    "\n",
    "doc = nlp(text)\n",
    "token_vectors = [t.vector for t in doc if not t.is_punct and not t.is_space and not t.is_stop]\n",
    "print(token_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vectors[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice immediately that the vectors are not 0's and 1's anymore.\n",
    "\n",
    "This is because the million words corpuse used to train the model, has been reduced to (in this case) 96 values by using dimentionality reduction techniques like PCA.\n",
    "\n",
    "\n",
    "Embeddings now represent the words in a 96 dimensions space.\n",
    "\n",
    "Similar words should be found in a similar area in space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings with Word2Vec\n",
    "\n",
    "Word2Vec is an embeddings model that can be used to train on your own corpus of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some logging definition so as to be able to trace what's going on under the hood\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/emmanuel/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "# model = Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "#     brown.sents(),\n",
    "#     min_count=10,   # minimum frequency of token\n",
    "    window=5,   # how many words at a time when scanning a sentence (more on this later)\n",
    "#     size=300,   # size of the output vector\n",
    "#     sample=6e-5,  # sampleing noise factor\n",
    "#     alpha=0.03,  # learning rate\n",
    "#     min_alpha=0.0007, \n",
    "#     negative=20,  #\n",
    "    workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-23 12:30:23,973 : INFO : collecting all words and their counts\n",
      "2019-07-23 12:30:23,975 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-07-23 12:30:24,822 : INFO : PROGRESS: at sentence #10000, processed 219770 words, keeping 23488 word types\n",
      "2019-07-23 12:30:25,743 : INFO : PROGRESS: at sentence #20000, processed 430477 words, keeping 34367 word types\n",
      "2019-07-23 12:30:26,632 : INFO : PROGRESS: at sentence #30000, processed 669056 words, keeping 42365 word types\n",
      "2019-07-23 12:30:27,336 : INFO : PROGRESS: at sentence #40000, processed 888291 words, keeping 49136 word types\n",
      "2019-07-23 12:30:27,936 : INFO : PROGRESS: at sentence #50000, processed 1039920 words, keeping 53024 word types\n",
      "2019-07-23 12:30:28,398 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 57340 sentences\n",
      "2019-07-23 12:30:28,399 : INFO : Loading a fresh vocabulary\n",
      "2019-07-23 12:30:28,500 : INFO : effective_min_count=5 retains 15173 unique words (27% of original 56057, drops 40884)\n",
      "2019-07-23 12:30:28,501 : INFO : effective_min_count=5 leaves 1095086 word corpus (94% of original 1161192, drops 66106)\n",
      "2019-07-23 12:30:28,599 : INFO : deleting the raw counts dictionary of 56057 items\n",
      "2019-07-23 12:30:28,602 : INFO : sample=0.001 downsamples 42 most-common words\n",
      "2019-07-23 12:30:28,604 : INFO : downsampling leaves estimated 781596 word corpus (71.4% of prior 1095086)\n",
      "2019-07-23 12:30:28,693 : INFO : estimated required memory for 15173 words and 100 dimensions: 19724900 bytes\n",
      "2019-07-23 12:30:28,694 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(brown.sents(), progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-23 12:30:37,331 : INFO : training model with 4 workers on 15173 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-07-23 12:30:38,343 : INFO : EPOCH 1 - PROGRESS: at 17.44% examples, 145236 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:39,382 : INFO : EPOCH 1 - PROGRESS: at 36.33% examples, 146911 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:40,389 : INFO : EPOCH 1 - PROGRESS: at 49.43% examples, 137679 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:41,437 : INFO : EPOCH 1 - PROGRESS: at 67.44% examples, 142224 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:42,482 : INFO : EPOCH 1 - PROGRESS: at 88.13% examples, 137078 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:43,005 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-23 12:30:43,007 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-23 12:30:43,008 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-23 12:30:43,011 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-23 12:30:43,013 : INFO : EPOCH - 1 : training on 1161192 raw words (781703 effective words) took 5.7s, 137688 effective words/s\n",
      "2019-07-23 12:30:44,046 : INFO : EPOCH 2 - PROGRESS: at 18.20% examples, 149586 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:45,055 : INFO : EPOCH 2 - PROGRESS: at 36.33% examples, 147854 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:46,073 : INFO : EPOCH 2 - PROGRESS: at 55.32% examples, 155571 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:47,080 : INFO : EPOCH 2 - PROGRESS: at 78.69% examples, 160406 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:47,910 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-23 12:30:47,911 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-23 12:30:47,913 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-23 12:30:47,915 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-23 12:30:47,915 : INFO : EPOCH - 2 : training on 1161192 raw words (782023 effective words) took 4.9s, 159747 effective words/s\n",
      "2019-07-23 12:30:48,965 : INFO : EPOCH 3 - PROGRESS: at 19.95% examples, 160795 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:49,968 : INFO : EPOCH 3 - PROGRESS: at 31.82% examples, 127920 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:50,983 : INFO : EPOCH 3 - PROGRESS: at 45.19% examples, 124551 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:51,992 : INFO : EPOCH 3 - PROGRESS: at 61.85% examples, 132007 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:53,021 : INFO : EPOCH 3 - PROGRESS: at 88.13% examples, 138492 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:53,523 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-23 12:30:53,524 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-23 12:30:53,526 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-23 12:30:53,529 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-23 12:30:53,532 : INFO : EPOCH - 3 : training on 1161192 raw words (781609 effective words) took 5.6s, 139462 effective words/s\n",
      "2019-07-23 12:30:54,541 : INFO : EPOCH 4 - PROGRESS: at 19.05% examples, 159374 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:55,608 : INFO : EPOCH 4 - PROGRESS: at 39.31% examples, 158009 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:56,648 : INFO : EPOCH 4 - PROGRESS: at 53.82% examples, 148344 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:57,680 : INFO : EPOCH 4 - PROGRESS: at 76.21% examples, 153805 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:30:58,585 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-23 12:30:58,586 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-23 12:30:58,588 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-23 12:30:58,590 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-23 12:30:58,591 : INFO : EPOCH - 4 : training on 1161192 raw words (781531 effective words) took 5.1s, 154643 effective words/s\n",
      "2019-07-23 12:30:59,634 : INFO : EPOCH 5 - PROGRESS: at 19.95% examples, 161610 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:31:00,641 : INFO : EPOCH 5 - PROGRESS: at 39.99% examples, 163604 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:31:01,643 : INFO : EPOCH 5 - PROGRESS: at 58.31% examples, 165054 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:31:02,644 : INFO : EPOCH 5 - PROGRESS: at 82.18% examples, 166133 words/s, in_qsize 0, out_qsize 0\n",
      "2019-07-23 12:31:03,651 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-23 12:31:03,652 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-23 12:31:03,653 : INFO : EPOCH 5 - PROGRESS: at 99.80% examples, 154423 words/s, in_qsize 1, out_qsize 1\n",
      "2019-07-23 12:31:03,654 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-23 12:31:03,656 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-23 12:31:03,657 : INFO : EPOCH - 5 : training on 1161192 raw words (782022 effective words) took 5.1s, 154631 effective words/s\n",
      "2019-07-23 12:31:03,658 : INFO : training on a 5805960 raw words (3908888 effective words) took 26.3s, 148486 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3908888, 5805960)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(brown.sents(), total_examples=model.corpus_count, epochs=5, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-23 12:31:13,211 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8763573169708252),\n",
       " ('girl', 0.870948851108551),\n",
       " ('boy', 0.8382799625396729),\n",
       " ('child', 0.7810909748077393),\n",
       " ('young', 0.7795072197914124),\n",
       " ('paradise', 0.7738704681396484),\n",
       " ('himself', 0.7477854490280151),\n",
       " ('person', 0.7475799918174744),\n",
       " ('good', 0.7463479042053223),\n",
       " ('old', 0.7426416873931885)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sold', 0.942639172077179),\n",
       " ('boyhood', 0.9409275054931641),\n",
       " ('Class', 0.9401830434799194)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman','king'], negative=['man'], topn = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     /Users/emmanuel/nltk_data...\n",
      "[nltk_data]   Unzipping models/word2vec_sample.zip.\n",
      "2019-07-23 12:32:28,595 : INFO : loading projection weights from /Users/emmanuel/nltk_data/models/word2vec_sample/pruned.word2vec.txt\n",
      "2019-07-23 12:32:49,944 : INFO : loaded (43981, 300) matrix from /Users/emmanuel/nltk_data/models/word2vec_sample/pruned.word2vec.txt\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from nltk.data import find\n",
    "nltk.download('word2vec_sample')\n",
    "\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2019-07-23 12:32:59,317 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.7664012908935547),\n",
       " ('boy', 0.6824869513511658),\n",
       " ('teenager', 0.6586930155754089),\n",
       " ('girl', 0.5921713709831238),\n",
       " ('robber', 0.5585117936134338),\n",
       " ('men', 0.5489763617515564),\n",
       " ('guy', 0.5420036315917969),\n",
       " ('person', 0.5342026948928833),\n",
       " ('gentleman', 0.5337991714477539),\n",
       " ('Man', 0.5316052436828613)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189673542976379),\n",
       " ('princess', 0.5902431011199951)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman','king'], negative=['man'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.141162  ,  0.0566339 ,  0.0150038 , -0.0359245 ,  0.038883  ,\n",
       "       -0.0178566 , -0.0857962 ,  0.0029849 ,  0.0621283 ,  0.00084198,\n",
       "        0.0124679 , -0.108196  , -0.0363472 , -0.0655094 , -0.044166  ,\n",
       "        0.0176453 , -0.0422641 ,  0.0256755 ,  0.0128906 , -0.0435321 ,\n",
       "       -0.0566339 ,  0.00056132,  0.0113057 , -0.117494  ,  0.027683  ,\n",
       "       -0.0828377 , -0.0338113 ,  0.112423  ,  0.162294  , -0.0196528 ,\n",
       "        0.0701585 ,  0.0591698 , -0.027683  , -0.0089283 , -0.0418415 ,\n",
       "        0.109887  ,  0.107351  , -0.0549434 ,  0.0310641 ,  0.138626  ,\n",
       "        0.0136302 , -0.0166943 ,  0.0917132 , -0.00351321,  0.0963622 ,\n",
       "       -0.0583245 , -0.032966  ,  0.0045434 , -0.0224    ,  0.016483  ,\n",
       "       -0.0579019 ,  0.0540981 ,  0.0241962 , -0.0790339 ,  0.0352906 ,\n",
       "       -0.0365585 , -0.0336    , -0.0188075 ,  0.0350792 , -0.0047283 ,\n",
       "        0.0756528 ,  0.132709  , -0.0187019 , -0.0061283 ,  0.0393056 ,\n",
       "       -0.00401509, -0.0148981 , -0.0498717 ,  0.0538868 , -0.0106717 ,\n",
       "        0.0369811 ,  0.0621283 , -0.12003   ,  0.0158491 , -0.0477585 ,\n",
       "        0.0566339 , -0.00744905, -0.0348679 , -0.00306415, -0.0127849 ,\n",
       "        0.130174  , -0.0390943 ,  0.0136302 , -0.0807245 , -0.0483924 ,\n",
       "        0.0443773 , -0.112423  , -0.00956226,  0.0726943 , -0.023034  ,\n",
       "       -0.0629736 , -0.0045434 , -0.0131547 , -0.0166943 ,  0.0498717 ,\n",
       "       -0.00270755, -0.0604377 ,  0.0346566 ,  0.0264151 , -0.0663547 ,\n",
       "       -0.0481811 , -0.061283  ,  0.0857962 , -0.0300075 ,  0.126792  ,\n",
       "       -0.0693132 ,  0.0904453 ,  0.00018078,  0.00792453, -0.0874868 ,\n",
       "        0.0206038 ,  0.0251472 , -0.00533585, -0.00861132, -0.0015783 ,\n",
       "       -0.00586415, -0.0357132 , -0.0136302 ,  0.00310377,  0.0870641 ,\n",
       "        0.0359245 , -0.0208151 , -0.114113  , -0.0418415 , -0.0976302 ,\n",
       "       -0.0418415 ,  0.0281057 , -0.0108302 ,  0.0367698 ,  0.0138415 ,\n",
       "       -0.0327547 , -0.109887  , -0.0505056 , -0.00626038, -0.0693132 ,\n",
       "       -0.00303774, -0.0247245 ,  0.0121509 , -0.0397283 ,  0.109041  ,\n",
       "        0.104392  ,  0.0287396 ,  0.0562113 ,  0.0739622 , -0.0591698 ,\n",
       "        0.0811471 , -0.0433207 , -0.0181736 , -0.0519849 ,  0.0022717 ,\n",
       "        0.0676226 , -0.0879094 , -0.0308528 , -0.0264151 ,  0.00707924,\n",
       "        0.0790339 ,  0.0155321 , -0.0183849 ,  0.0726943 , -0.0650868 ,\n",
       "       -0.0735396 ,  0.00792453,  0.12003   , -0.00549434, -0.0169057 ,\n",
       "       -0.0667773 ,  0.0798792 , -0.034234  ,  0.0390943 , -0.0117283 ,\n",
       "        0.0357132 ,  0.0291623 , -0.0697358 , -0.0849509 , -0.0659321 ,\n",
       "       -0.0203924 ,  0.0206038 ,  0.0255698 , -0.073117  , -0.0646641 ,\n",
       "        0.0562113 ,  0.0187019 ,  0.0113585 , -0.077766  , -0.0849509 ,\n",
       "        0.0278943 ,  0.0386717 ,  0.0710038 , -0.016483  , -0.0424755 ,\n",
       "       -0.00641887,  0.0811471 ,  0.055366  ,  0.0984754 ,  0.0078717 ,\n",
       "       -0.0344453 , -0.0492377 , -0.0300075 , -0.0672    , -0.0350792 ,\n",
       "       -0.0401509 , -0.0490264 , -0.0655094 , -0.0346566 , -0.0221887 ,\n",
       "       -0.0663547 ,  0.0505056 ,  0.0295849 ,  0.0140528 , -0.147079  ,\n",
       "       -0.0352906 , -0.0361358 ,  0.0173283 ,  0.0212377 , -0.104392  ,\n",
       "       -0.0583245 , -0.0256755 ,  0.0526189 , -0.147924  ,  0.0714264 ,\n",
       "        0.0267321 , -0.0786113 ,  0.0870641 , -0.0333887 ,  0.0710038 ,\n",
       "        0.0017434 ,  0.131019  , -0.0433207 , -0.0591698 , -0.0257811 ,\n",
       "        0.027049  , -0.0921358 , -0.028317  ,  0.077766  , -0.0336    ,\n",
       "       -0.00834717, -0.0650868 , -0.00045896,  0.0147924 ,  0.0144755 ,\n",
       "        0.083683  ,  0.00850566, -0.0862188 , -0.0460679 ,  0.00660377,\n",
       "        0.00401509,  0.0061283 , -0.0103547 ,  0.0254641 ,  0.0102491 ,\n",
       "        0.0540981 ,  0.0206038 , -0.0240906 ,  0.050083  ,  0.0638188 ,\n",
       "        0.0439547 , -0.0308528 , -0.0333887 , -0.0545207 ,  0.0126264 ,\n",
       "        0.0412075 , -0.0536755 , -0.0473358 , -0.0557887 ,  0.0705811 ,\n",
       "        0.122566  , -0.0136302 ,  0.0532528 , -0.100589  , -0.0405736 ,\n",
       "       -0.0562113 ,  0.00586415, -0.0168    , -0.0357132 ,  0.00388302,\n",
       "        0.0705811 ,  0.0456453 , -0.0600151 , -0.0714264 , -0.0166943 ,\n",
       "        0.0469132 , -0.0454339 ,  0.027683  ,  0.168211  , -0.0258868 ,\n",
       "       -0.0265207 , -0.0452226 , -0.0726943 ,  0.0310641 ,  0.0604377 ,\n",
       "        0.0672    , -0.0135245 , -0.0874868 , -0.0562113 , -0.0435321 ,\n",
       "       -0.027683  , -0.0361358 , -0.131019  , -0.0346566 ,  0.00908679],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['man']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words (BOW)\n",
    "\n",
    "Bag Of Words  is a method to create a defined-size vocabulary that ignores the word order. \n",
    "\n",
    "It creates a set of unique words, and words are vectorized by index in the vocabulary (as seen before)\n",
    "\n",
    "When looking for keywords or common terms, various scoring methods can be used.\n",
    "\n",
    "Term Frequency - Inverse Document Frequency (TF-IDF) is a common way to score terms in documents, \n",
    "by their frequency in the document, normalozed to the inverse frequency of the term across all documents in the corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unigrams, bigrams, trigrams, n-grams\n",
    "\n",
    "A single word token is a unigram\n",
    "\n",
    "A two word token is called a bigram\n",
    "\n",
    "A three word token is called a trigram\n",
    "\n",
    "So a n-word token is called a n-gram\n",
    "\n",
    "Using single words is often stripping the meaning of some combined words, so ngrams are useful, although they tend to have much lower frequencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note about Word2Vec\n",
    "\n",
    "Word2Vec uses 2 methods, one of which is called CBOW, for Continuous Bag Of Words, which uses a bag of words method\n",
    "scanning a window across the sentences used as input.\n",
    "\n",
    "This allows for preserving some of the relationship between words, and allows for clustering them into space by similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
